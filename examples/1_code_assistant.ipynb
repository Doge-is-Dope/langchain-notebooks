{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Sort the list based on the URLs and get the text\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix='# To build a RAG chain in LCEL, you need to define the components of the chain such as retrievers, tools, and chat models. Then, you can use the LangChain Expression Language (LCEL) to chain these components together to create a retrieval augmented generation (RAG) system.\\n\\n# Define the components of the RAG chain\\nretriever = ...\\ntool = ...\\nchat_model = ...\\n\\n# Chain the components together using LCEL\\nchain = {\\n    \"retriever\": retriever,\\n    \"tool\": tool,\\n    \"chat_model\": chat_model\\n}\\n\\n# Execute the RAG chain\\noutput = chain.invoke(input_data)\\n', imports='from langchain_core import ChatModel, Retriever, Tool', code='# Define the components of the RAG chain\\nretriever = Retriever(...)\\ntool = Tool(...)\\nchat_model = ChatModel(...)\\n\\n# Chain the components together using LCEL\\nchain = {\\n    \"retriever\": retriever,\\n    \"tool\": tool,\\n    \"chat_model\": chat_model\\n}\\n\\n# Execute the RAG chain\\noutput = chain.invoke(input_data)\\n', description='Schema for code solutions to questions about LCEL.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "### OpenAI\n",
    "\n",
    "# Grader prompt\n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language.\n",
    "                Here is a full set of LCEL documentation:\n",
    "                ------- \n",
    "                {context}\n",
    "                ------- \n",
    "                Answer the user question based on the above provided documentation. Ensure any code you provide can be executed \n",
    "                with all required imports and variables defined. Structure your answer with a description of the code solution.\n",
    "                Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Code output\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "    description = \"Schema for code solutions to questions about LCEL.\"\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "code_gen_chain = code_gen_prompt | llm.with_structured_output(code)\n",
    "question = \"How do I build a RAG chain in LCEL?\"\n",
    "solution = code_gen_chain.invoke(\n",
    "    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",
    ")\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        error : Binary flag for control flow to indicate whether test error was tripped\n",
    "        messages : With user question, error messages, reasoning\n",
    "        generation : Code solution\n",
    "        iterations : Number of tries\n",
    "    \"\"\"\n",
    "\n",
    "    error: str\n",
    "    messages: List\n",
    "    generation: str\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "### Parameter\n",
    "\n",
    "# Max tries\n",
    "max_iterations = 3\n",
    "# Reflect\n",
    "# flag = 'reflect'\n",
    "flag = \"do not reflect\"\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate a code solution\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    error = state[\"error\"]\n",
    "\n",
    "    # We have been routed back to generation with an error\n",
    "    if error == \"yes\":\n",
    "        messages += [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\",\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Solution\n",
    "    code_solution = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Increment\n",
    "    iterations = iterations + 1\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "def code_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    Check code\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECKING CODE---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    # Get solution components\n",
    "    imports = code_solution.imports\n",
    "    code = code_solution.code\n",
    "\n",
    "    # Check imports\n",
    "    try:\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # Check execution\n",
    "    try:\n",
    "        exec(imports + \"\\n\" + code)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # No errors\n",
    "    print(\"---NO CODE TEST FAILURES---\")\n",
    "    return {\n",
    "        \"generation\": code_solution,\n",
    "        \"messages\": messages,\n",
    "        \"iterations\": iterations,\n",
    "        \"error\": \"no\",\n",
    "    }\n",
    "\n",
    "\n",
    "def reflect(state: GraphState):\n",
    "    \"\"\"\n",
    "    Reflect on errors\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "\n",
    "    # Prompt reflection\n",
    "\n",
    "    # Add reflection\n",
    "    reflections = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_finish(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether to finish.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    if error == \"no\" or iterations == max_iterations:\n",
    "        print(\"---DECISION: FINISH---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        if flag == \"reflect\":\n",
    "            return \"reflect\"\n",
    "        else:\n",
    "            return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"generate\", generate)  # generation solution\n",
    "workflow.add_node(\"check_code\", code_check)  # check code\n",
    "workflow.add_node(\"reflect\", reflect)  # reflect\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"generate\")\n",
    "workflow.add_edge(\"generate\", \"check_code\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_code\",\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"reflect\": \"reflect\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"reflect\", \"generate\")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'error': 'no',\n",
       " 'messages': [('user',\n",
       "   'How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?'),\n",
       "  ('assistant',\n",
       "   'To directly pass a string to a runnable and use it to construct the input needed for a prompt, you can create a ChatPromptTemplate with the string as the input and then invoke the prompt with the string. \\n Imports: from langchain_core.prompts import ChatPromptTemplate \\n Code: # Create a ChatPromptTemplate with the string\\nprompt = ChatPromptTemplate.from_template(\"Your string goes here\")\\n\\n# Invoke the prompt with the string\\nprompt.invoke({})')],\n",
       " 'generation': code(prefix='To directly pass a string to a runnable and use it to construct the input needed for a prompt, you can create a ChatPromptTemplate with the string as the input and then invoke the prompt with the string.', imports='from langchain_core.prompts import ChatPromptTemplate', code='# Create a ChatPromptTemplate with the string\\nprompt = ChatPromptTemplate.from_template(\"Your string goes here\")\\n\\n# Invoke the prompt with the string\\nprompt.invoke({})', description='Code solution to directly pass a string to a runnable and use it to construct the input needed for a prompt.'),\n",
       " 'iterations': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\n",
    "app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith\n",
    "\n",
    "client = langsmith.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the dataset to your tenant to use it\n",
    "public_dataset = (\n",
    "    \"https://smith.langchain.com/public/326674a6-62bd-462d-88ae-eea49d503f9d/d\"\n",
    ")\n",
    "client.clone_public_dataset(public_dataset, dataset_name=\"test-LCEL-code-gen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "def check_import(run: Run, example: Example) -> dict:\n",
    "    imports = run.outputs.get(\"imports\")\n",
    "    try:\n",
    "        exec(imports)\n",
    "        return {\"key\": \"import_check\", \"score\": 1}\n",
    "    except Exception:\n",
    "        return {\"key\": \"import_check\", \"score\": 0}\n",
    "\n",
    "\n",
    "def check_execution(run: Run, example: Example) -> dict:\n",
    "    imports = run.outputs.get(\"imports\")\n",
    "    code = run.outputs.get(\"code\")\n",
    "    try:\n",
    "        exec(imports + \"\\n\" + code)\n",
    "        return {\"key\": \"code_execution_check\", \"score\": 1}\n",
    "    except Exception:\n",
    "        return {\"key\": \"code_execution_check\", \"score\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_base_case(example: dict):\n",
    "    \"\"\"Context stuffing\"\"\"\n",
    "    solution = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": [(\"user\", example[\"question\"])]}\n",
    "    )\n",
    "    solution_structured = code_gen_chain.invoke([(\"code\", solution)])\n",
    "    return {\"imports\": solution_structured.imports, \"code\": solution_structured.code}\n",
    "\n",
    "\n",
    "def predict_langgraph(example: dict):\n",
    "    \"\"\"LangGraph\"\"\"\n",
    "    graph = app.invoke({\"messages\": [(\"user\", example[\"question\"])], \"iterations\": 0})\n",
    "    solution = graph[\"generation\"]\n",
    "    return {\"imports\": solution.imports, \"code\": solution.code}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Evaluator\n",
    "code_evalulator = [check_import, check_execution]\n",
    "\n",
    "# Dataset\n",
    "dataset_name = \"test-LCEL-code-gen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-without-langgraph-client=<openai.resources.chat.completions.Completions object at 0x11b25f050> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x11b277150> temperature=0.0 openai_api_key=SecretStr('**********') openai_proxy=''-e4391c19' at:\n",
      "https://smith.langchain.com/o/0feae8f0-454e-5e76-8ccc-bd3eb6e53859/datasets/081189c4-216b-49f6-ba9f-c4e362a4dfe0/compare?selectedSessions=0f3aec20-4ced-4308-b091-09d343cdde55\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3baa0dcc01494d35aada2ac8bebc2faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 53743, Requested 17636. Please try again in 11.379s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 56323, Requested 17656. Please try again in 13.979s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 56812, Requested 17645. Please try again in 14.457s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 56573, Requested 17657. Please try again in 14.23s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 57561, Requested 17638. Please try again in 15.199s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "# Run base case\n",
    "experiment_results_ = evaluate(\n",
    "    predict_base_case,\n",
    "    data=dataset_name,\n",
    "    evaluators=code_evalulator,\n",
    "    experiment_prefix=f\"test-without-langgraph-{llm}\",\n",
    "    max_concurrency=2,\n",
    "    metadata={\n",
    "        \"llm\": llm,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-with-langgraph-client=<openai.resources.chat.completions.Completions object at 0x11b25f050> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x11b277150> temperature=0.0 openai_api_key=SecretStr('**********') openai_proxy=''-do not reflect-785738f9' at:\n",
      "https://smith.langchain.com/o/0feae8f0-454e-5e76-8ccc-bd3eb6e53859/datasets/081189c4-216b-49f6-ba9f-c4e362a4dfe0/compare?selectedSessions=a864fb85-5e6a-4872-be0d-af7e61afbf8f\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545e984eb5cf413f8983e3759e549f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 52151, Requested 17881. Please try again in 10.032s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 56460, Requested 17637. Please try again in 14.097s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 56157, Requested 17643. Please try again in 13.8s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 57320, Requested 17632. Please try again in 14.952s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 52751, Requested 17656. Please try again in 10.407s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 52447, Requested 17801. Please try again in 10.248s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 52404, Requested 17634. Please try again in 10.038s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 52215, Requested 17979. Please try again in 10.194s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 55975, Requested 17657. Please try again in 13.632s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 56540, Requested 17945. Please try again in 14.485s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 52698, Requested 17638. Please try again in 10.336s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-g7OThWZZWSYLozKLfRKTmdst on tokens per min (TPM): Limit 60000, Used 53040, Requested 17846. Please try again in 10.886s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "# Run with langgraph\n",
    "experiment_results = evaluate(\n",
    "    predict_langgraph,\n",
    "    data=dataset_name,\n",
    "    evaluators=code_evalulator,\n",
    "    experiment_prefix=f\"test-with-langgraph-{llm}-{flag}\",\n",
    "    max_concurrency=2,\n",
    "    metadata={\n",
    "        \"llm\": llm,\n",
    "        \"feedback\": flag,\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
