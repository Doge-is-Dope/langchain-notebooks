{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMCompiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults(\n",
    "    max_results=1,\n",
    "    description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n",
    ")\n",
    "\n",
    "tools = [search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.output_parsers.transform import BaseTransformOutputParser\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import BaseTool\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "THOUGHT_PATTERN = r\"Thought: ([^\\n]*)\"\n",
    "ACTION_PATTERN = r\"\\n*(\\d+)\\. (\\w+)\\((.*)\\)(\\s*#\\w+\\n)?\"\n",
    "# $1 or ${1} -> 1\n",
    "ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
    "END_OF_PLAN = \"<END_OF_PLAN>\"\n",
    "\n",
    "\n",
    "### Helper functions\n",
    "\n",
    "\n",
    "def _ast_parse(arg: str) -> Any:\n",
    "    try:\n",
    "        return ast.literal_eval(arg)\n",
    "    except:  # noqa\n",
    "        return arg\n",
    "\n",
    "\n",
    "def _parse_llm_compiler_action_args(args: str, tool: Union[str, BaseTool]) -> list[Any]:\n",
    "    \"\"\"Parse arguments from a string.\"\"\"\n",
    "    if args == \"\":\n",
    "        return ()\n",
    "    if isinstance(tool, str):\n",
    "        return ()\n",
    "    extracted_args = {}\n",
    "    tool_key = None\n",
    "    prev_idx = None\n",
    "    for key in tool.args.keys():\n",
    "        # Split if present\n",
    "        if f\"{key}=\" in args:\n",
    "            idx = args.index(f\"{key}=\")\n",
    "            if prev_idx is not None:\n",
    "                extracted_args[tool_key] = _ast_parse(\n",
    "                    args[prev_idx:idx].strip().rstrip(\",\")\n",
    "                )\n",
    "            args = args.split(f\"{key}=\", 1)[1]\n",
    "            tool_key = key\n",
    "            prev_idx = 0\n",
    "    if prev_idx is not None:\n",
    "        extracted_args[tool_key] = _ast_parse(\n",
    "            args[prev_idx:].strip().rstrip(\",\").rstrip(\")\")\n",
    "        )\n",
    "    return extracted_args\n",
    "\n",
    "\n",
    "def default_dependency_rule(idx, args: str):\n",
    "    matches = re.findall(ID_PATTERN, args)\n",
    "    numbers = [int(match) for match in matches]\n",
    "    return idx in numbers\n",
    "\n",
    "\n",
    "def _get_dependencies_from_graph(\n",
    "    idx: int, tool_name: str, args: Dict[str, Any]\n",
    ") -> dict[str, list[str]]:\n",
    "    \"\"\"Get dependencies from a graph.\"\"\"\n",
    "    if tool_name == \"join\":\n",
    "        return list(range(1, idx))\n",
    "    return [i for i in range(1, idx) if default_dependency_rule(i, str(args))]\n",
    "\n",
    "\n",
    "class Task(TypedDict):\n",
    "    idx: int\n",
    "    tool: BaseTool\n",
    "    args: list\n",
    "    dependencies: Dict[str, list]\n",
    "    thought: Optional[str]\n",
    "\n",
    "\n",
    "def instantiate_task(\n",
    "    tools: Sequence[BaseTool],\n",
    "    idx: int,\n",
    "    tool_name: str,\n",
    "    args: Union[str, Any],\n",
    "    thought: Optional[str] = None,\n",
    ") -> Task:\n",
    "    if tool_name == \"join\":\n",
    "        tool = \"join\"\n",
    "    else:\n",
    "        try:\n",
    "            tool = tools[[tool.name for tool in tools].index(tool_name)]\n",
    "        except ValueError as e:\n",
    "            raise OutputParserException(f\"Tool {tool_name} not found.\") from e\n",
    "    tool_args = _parse_llm_compiler_action_args(args, tool)\n",
    "    dependencies = _get_dependencies_from_graph(idx, tool_name, tool_args)\n",
    "\n",
    "    return Task(\n",
    "        idx=idx,\n",
    "        tool=tool,\n",
    "        args=tool_args,\n",
    "        dependencies=dependencies,\n",
    "        thought=thought,\n",
    "    )\n",
    "\n",
    "\n",
    "class LLMCompilerPlanParser(BaseTransformOutputParser[dict], extra=\"allow\"):\n",
    "    \"\"\"Planning output parser.\"\"\"\n",
    "\n",
    "    tools: List[BaseTool]\n",
    "\n",
    "    def _transform(self, input: Iterator[Union[str, BaseMessage]]) -> Iterator[Task]:\n",
    "        texts = []\n",
    "        # TODO: Cleanup tuple state tracking here.\n",
    "        thought = None\n",
    "        for chunk in input:\n",
    "            # Assume input is str. TODO: support vision/other formats\n",
    "            text = chunk if isinstance(chunk, str) else str(chunk.content)\n",
    "            for task, thought in self.ingest_token(text, texts, thought):\n",
    "                yield task\n",
    "        # Final possible task\n",
    "        if texts:\n",
    "            task, _ = self._parse_task(\"\".join(texts), thought)\n",
    "            if task:\n",
    "                yield task\n",
    "\n",
    "    def parse(self, text: str) -> List[Task]:\n",
    "        return list(self._transform([text]))\n",
    "\n",
    "    def stream(\n",
    "        self,\n",
    "        input: str | BaseMessage,\n",
    "        config: RunnableConfig | None = None,\n",
    "        **kwargs: Any | None,\n",
    "    ) -> Iterator[Task]:\n",
    "        yield from self.transform([input], config, **kwargs)\n",
    "\n",
    "    def ingest_token(\n",
    "        self, token: str, buffer: List[str], thought: Optional[str]\n",
    "    ) -> Iterator[Tuple[Optional[Task], str]]:\n",
    "        buffer.append(token)\n",
    "        if \"\\n\" in token:\n",
    "            buffer_ = \"\".join(buffer).split(\"\\n\")\n",
    "            suffix = buffer_[-1]\n",
    "            for line in buffer_[:-1]:\n",
    "                task, thought = self._parse_task(line, thought)\n",
    "                if task:\n",
    "                    yield task, thought\n",
    "            buffer.clear()\n",
    "            buffer.append(suffix)\n",
    "\n",
    "    def _parse_task(self, line: str, thought: Optional[str] = None):\n",
    "        task = None\n",
    "        if match := re.match(THOUGHT_PATTERN, line):\n",
    "            # Optionally, action can be preceded by a thought\n",
    "            thought = match.group(1)\n",
    "        elif match := re.match(ACTION_PATTERN, line):\n",
    "            # if action is parsed, return the task, and clear the buffer\n",
    "            idx, tool_name, args, _ = match.groups()\n",
    "            idx = int(idx)\n",
    "            task = instantiate_task(\n",
    "                tools=self.tools,\n",
    "                idx=idx,\n",
    "                tool_name=tool_name,\n",
    "                args=args,\n",
    "                thought=thought,\n",
    "            )\n",
    "            thought = None\n",
    "        # Else it is just dropped\n",
    "        return task, thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following \u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m types:\n",
      "\u001b[33;1m\u001b[1;3m{tool_descriptions}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m. join(): Collects and combines results from prior actions.\n",
      "\n",
      " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
      " - join should always be the last action in the plan, and will be called in two scenarios:\n",
      "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
      "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
      " - Each action described above contains input/output types and description.\n",
      "    - You must strictly adhere to the input and output types for each action.\n",
      "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
      " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
      " - Each action MUST have a unique ID, which is strictly increasing.\n",
      " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
      " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
      " - Ensure the plan maximizes parallelizability.\n",
      " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
      " - Never introduce new actions other than the ones provided.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n",
      "\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Remember, ONLY respond with the task list in the correct format! E.g.:\n",
      "idx. tool(arg_name=args)\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clement/Developer/ai/langchain-notebooks/.venv/lib/python3.11/site-packages/langsmith/client.py:5434: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = hub.pull(\"wfh/llm-compiler\")\n",
    "print(prompt.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_planner(\n",
    "    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate\n",
    "):\n",
    "    tool_descriptions = \"\\n\".join(\n",
    "        f\"{i+1}. {tool.description}\\n\"\n",
    "        for i, tool in enumerate(\n",
    "            tools\n",
    "        )  # +1 to offset the 0 starting index, we want it count normally from 1.\n",
    "    )\n",
    "    planner_prompt = base_prompt.partial(\n",
    "        replan=\"\",\n",
    "        num_tools=len(tools)\n",
    "        + 1,  # Add one because we're adding the join() tool at the end.\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "    replanner_prompt = base_prompt.partial(\n",
    "        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
    "        \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n",
    "        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
    "        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
    "        \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n",
    "        \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\",\n",
    "        num_tools=len(tools) + 1,\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "\n",
    "    def should_replan(state: list):\n",
    "        # Context is passed as a system message\n",
    "        return isinstance(state[-1], SystemMessage)\n",
    "\n",
    "    def wrap_messages(state: list):\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    def wrap_and_get_last_index(state: list):\n",
    "        next_task = 0\n",
    "        for message in state[::-1]:\n",
    "            if isinstance(message, FunctionMessage):\n",
    "                next_task = message.additional_kwargs[\"idx\"] + 1\n",
    "                break\n",
    "        state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    return (\n",
    "        RunnableBranch(\n",
    "            (should_replan, wrap_and_get_last_index | replanner_prompt),\n",
    "            wrap_messages | planner_prompt,\n",
    "        )\n",
    "        | llm\n",
    "        | LLMCompilerPlanParser(tools=tools)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# This is the primary \"agent\" in our application\n",
    "planner = create_planner(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description='tavily_search_results_json(query=\"the search query\") - a search engine.' max_results=1 {'query': 'current temperature in San Francisco'}\n",
      "---\n",
      "join ()\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "example_question = \"What's the temperature in SF raised to the 3rd power?\"\n",
    "\n",
    "for task in planner.stream([HumanMessage(content=example_question)]):\n",
    "    print(task[\"tool\"], task[\"args\"])\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
