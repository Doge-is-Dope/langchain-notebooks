{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T14:46:30.289974Z",
     "start_time": "2024-06-29T14:46:30.286812Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Langchain configuration\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# OpenAI API Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in Functions\n",
    "\n",
    "The built-in chains, `create_stuff_documents_chain` and `create_retrieval_chain` allow us to craft a retrieval pipeline with following ingredients:\n",
    "\n",
    "- retriever: This can be created from a vector store\n",
    "- prompt: This is the query and the placeholder for the context\n",
    "- LLM: This is the language model that will be used to answer the question based on the context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T14:46:32.343678Z",
     "start_time": "2024-06-29T14:46:30.291230Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=[\"https://lilianweng.github.io/posts/2023-06-23-agent/\"],\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the documents into chunks.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a retriever from the documents.\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# 2. Incorporate the retriever into a question-answering chain.\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the model\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Create the chains\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T14:46:34.172182Z",
     "start_time": "2024-06-29T14:46:32.344720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition involves breaking down a complex task into smaller and simpler steps or subgoals. This approach allows for better utilization of computational resources and aids in enhancing model performance on difficult tasks. Models are guided to \"think step by step\" to transform big tasks into more manageable components, providing insights into the model\\'s thinking process.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T14:46:35.892204Z",
     "start_time": "2024-06-29T14:46:34.172778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Common ways of integrating reasoning and acting within large language models (LLMs) include extending the action space to combine task-specific discrete actions and the language space. This allows LLMs to interact with the environment and generate reasoning traces in natural language. Additionally, using tool APIs, such as in ChatGPT Plugins and OpenAI API function calling, can augment LLMs with tool use capability for practical applications.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What are common ways of doing it?\"})\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History\n",
    "\n",
    "> Human: \"What is Task Decomposition?\"\n",
    ">\n",
    "> AI: \"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\"\n",
    ">\n",
    "> Human: \"What are common ways of doing it?\"\n",
    "\n",
    "To answer the second question, the AI needs to know what `it` refers to. So we need to add two things:\n",
    "\n",
    "- Prompt: Update our prompt to support historical messages as an input\n",
    "- Contextualizing questions: Add a sub-chain that takes the latest user question and reformulates it in the context of the chat history. That is, the pipeline will be changed:\n",
    "\n",
    "Before:\n",
    "query -> retriever\n",
    "After:\n",
    "(query, conversation history) -> LLM -> rephrased query -> retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T14:46:35.898438Z",
     "start_time": "2024-06-29T14:46:35.894634Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T14:46:35.903347Z",
     "start_time": "2024-06-29T14:46:35.899752Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T14:46:40.013206Z",
     "start_time": "2024-06-29T14:46:35.904613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common ways of task decomposition include using techniques like Chain of Thought (CoT) or Tree of Thoughts, where models are guided to break down complex tasks into smaller steps. This can be achieved through simple prompting with language models, task-specific instructions tailored to the particular task at hand, or with human inputs to guide the decomposition process effectively.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory\n",
    "\n",
    "Manually store the conversion history is easy but not scalable. We can use the `memory` chain to store the conversation history **automatically** and use it in the retrieval pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T14:46:40.016753Z",
     "start_time": "2024-06-29T14:46:40.013999Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T14:46:41.857719Z",
     "start_time": "2024-06-29T14:46:40.017762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps, making it easier for an agent or model to handle. This process allows for tasks to be transformed into multiple manageable subtasks, enhancing performance on difficult tasks by guiding the model to think step by step. It involves instructing the model to utilize additional computation at test time to effectively decompose hard tasks.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T14:46:44.408491Z",
     "start_time": "2024-06-29T14:46:41.858456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition can be achieved through various methods such as using Chain of Thought (CoT) prompting techniques, which involve instructing the model to \"think step by step\" to break down tasks. Other common ways include utilizing Large Language Models (LLM) with simple prompts, task-specific instructions tailored to the particular task at hand, or incorporating human inputs to guide the decomposition process effectively.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the history via `session_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T14:46:44.417764Z",
     "start_time": "2024-06-29T14:46:44.415404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is Task Decomposition?\n",
      "\n",
      "AI: Task decomposition is a technique used to break down complex tasks into smaller and simpler steps, making it easier for an agent or model to handle. This process allows for tasks to be transformed into multiple manageable subtasks, enhancing performance on difficult tasks by guiding the model to think step by step. It involves instructing the model to utilize additional computation at test time to effectively decompose hard tasks.\n",
      "\n",
      "User: What are common ways of doing it?\n",
      "\n",
      "AI: Task decomposition can be achieved through various methods such as using Chain of Thought (CoT) prompting techniques, which involve instructing the model to \"think step by step\" to break down tasks. Other common ways include utilizing Large Language Models (LLM) with simple prompts, task-specific instructions tailored to the particular task at hand, or incorporating human inputs to guide the decomposition process effectively.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
